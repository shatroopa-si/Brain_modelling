{
    "collab_server" : "",
    "contents" : "clusteringVector = vector(length = nPoints)     # which data point was allotted which cluster\n\n## analyzing pure data\nanalysis <- function(dataset)\n{\n  mapCluster = vector(length = k)\n  \n  centroids = initCentroid(dataset, k)\n  #clusters = createClusters(dataset, k, 50, centroids, clusteringVector)\n  centroids = createClusters(dataset, k, 50, centroids, clusteringVector)\n  \n  # print(\"::::::Final centroids:::::\")\n  # print(centroids)\n  \n  ##create a map in cluster numbers and apply the map on clustering vector\n  mapCluster[clusteringVector[1]] = expectedClusteringVector[1]\n  mapCluster[3 - clusteringVector[1]] = 3 - expectedClusteringVector[1]\n  for(i in 1:nPoints)\n  {\n    clusteringVector[i] = mapCluster[clusteringVector[i]]\n  }\n  \n  \n  ##compare\n  matches = countTrue(clusteringVector == expectedClusteringVector)\n  cat(\"Matches = \", matches)\n  cat(\"\\n\")\n  efficiency = (matches / nPoints * 1.0) * 100\n  cat(\"Efficiency = \", efficiency)\n  cat(\"\\n\")\n  \n  # ##silhouette plot with built-in k\n  # library(cluster)\n  # silhouettePlot = silhouette(dataset, dist(dataset))\n  # plot(silhouettePlot, col = 1:k)\n  \n  return(centroids)\n}\n\n\n## analyze pure data------------------------------------------------------------\nprint(\":::::::::::::::::Pure data analysis:::::::::::::::::\")\ncentroidsPureData = analysis(pureDataDict)\n\n##test--\n#load file\ntestFileName = \"testData.txt\"\ntestDataDict = read.table(testFileName, sep = \" \", fill = FALSE, strip.white = TRUE)\nnTestPoints = nrow(testDataDict)\nexpectedTestClusteringVector = testDataDict[[dimension + 1]]\nexpectedTestClusteringVector = expectedTestClusteringVector + 1\ntestDataDict = testDataDict[, 1:dimension]\n\n##assign clusters to each point\ntestClusteringVector = vector(length = nTestPoints)\nfor(i in 1:nTestPoints)\n{\n  point = retreivePoint(testDataDict, i)\n  \n  ##calculate distance from each cluster and find the minimum\n  dist = vector(length = k)\n  for(j in 1:k)\n  {\n    dist[j] = euclidDistance(point, centroidsPureData[[j]])\n  }\n  minDist = min(dist)\n  \n  ##assign the point to the particular cluster\n  x = which(minDist == dist)[[1]]               #index of the cluster with minimum distance from the point\n  #clusters[[x]] = rbind(clusters[[x]], point)\n  testClusteringVector[i] <- x                     #assignment takes place in the global environment\n}\n\n##check\nmapCluster = vector(length = k)\nmapCluster[testClusteringVector[1]] = expectedTestClusteringVector[1]\nmapCluster[3 - testClusteringVector[1]] = 3 - expectedTestClusteringVector[1]\nfor(i in 1:nTestPoints)\n{\n  testClusteringVector[i] = mapCluster[testClusteringVector[i]]\n}\nmatches = countTrue(testClusteringVector == expectedTestClusteringVector)\ncat(\"Matches = \", matches)\ncat(\"\\n\")\nefficiency = (matches / nTestPoints * 1.0) * 100\ncat(\"Efficiency = \", efficiency)\ncat(\"\\n\")\n\n\n## analyze noisy data-----------------------------------------------------------\nprint(\":::::::::::::::::Noisy data analysis:::::::::::::::::\")\ncentroidsNoisyData = analysis(noisyDataDict)\n\n##test--\n##assign clusters to each point\ntestClusteringVector = vector(length = nTestPoints)\nfor(i in 1:nTestPoints)\n{\n  point = retreivePoint(testDataDict, i)\n  \n  ##calculate distance from each cluster and find the minimum\n  dist = vector(length = k)\n  for(j in 1:k)\n  {\n    dist[j] = euclidDistance(point, centroidsNoisyData[[j]])\n  }\n  minDist = min(dist)\n  \n  ##assign the point to the particular cluster\n  x = which(minDist == dist)[[1]]               #index of the cluster with minimum distance from the point\n  #clusters[[x]] = rbind(clusters[[x]], point)\n  testClusteringVector[i] <- x                     #assignment takes place in the global environment\n}\n\n##check\nmapCluster = vector(length = k)\nmapCluster[testClusteringVector[1]] = expectedTestClusteringVector[1]\nmapCluster[3 - testClusteringVector[1]] = 3 - expectedTestClusteringVector[1]\nfor(i in 1:nTestPoints)\n{\n  testClusteringVector[i] = mapCluster[testClusteringVector[i]]\n}\nmatches = countTrue(testClusteringVector == expectedTestClusteringVector)\ncat(\"Matches = \", matches)\ncat(\"\\n\")\nefficiency = (matches / nTestPoints * 1.0) * 100\ncat(\"Efficiency = \", efficiency)\ncat(\"\\n\")\n\n##introduce gaussian noise==========increase noise power\n",
    "created" : 1481038222348.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1169650548",
    "id" : "6D0D1163",
    "lastKnownWriteTime" : 1481103815,
    "last_content_update" : 1481103815678,
    "path" : "~/Desktop/cHEMICAL lOCHA/imp/brainModelling/kMeans/dataAnalysis.R",
    "project_path" : "dataAnalysis.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 5,
    "source_on_save" : true,
    "source_window" : "",
    "type" : "r_source"
}